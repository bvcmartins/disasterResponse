{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/brunom/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/brunom/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/brunom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///data/disasterResponse.db')\n",
    "df = pd.read_sql('SELECT * FROM messages_categories', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>buildings</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>clothing</th>\n",
       "      <th>cold</th>\n",
       "      <th>death</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>electricity</th>\n",
       "      <th>fire</th>\n",
       "      <th>floods</th>\n",
       "      <th>food</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>infrastructure_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>military</th>\n",
       "      <th>missing_people</th>\n",
       "      <th>money</th>\n",
       "      <th>offer</th>\n",
       "      <th>other_aid</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>refugees</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>shelter</th>\n",
       "      <th>shops</th>\n",
       "      <th>storm</th>\n",
       "      <th>tools</th>\n",
       "      <th>transport</th>\n",
       "      <th>water</th>\n",
       "      <th>weather_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre aid_centers  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct           0   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct           0   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct           0   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct           0   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct           0   \n",
       "\n",
       "  aid_related buildings child_alone clothing cold death direct_report  \\\n",
       "0           0         0           0        0    0     0             0   \n",
       "1           1         0           0        0    0     0             0   \n",
       "2           0         0           0        0    0     0             0   \n",
       "3           1         1           0        0    0     0             0   \n",
       "4           0         0           0        0    0     0             0   \n",
       "\n",
       "  earthquake electricity fire floods food hospitals infrastructure_related  \\\n",
       "0          0           0    0      0    0         0                      0   \n",
       "1          0           0    0      0    0         0                      0   \n",
       "2          0           0    0      0    0         0                      0   \n",
       "3          0           0    0      0    0         1                      1   \n",
       "4          0           0    0      0    0         0                      0   \n",
       "\n",
       "  medical_help medical_products military missing_people money offer other_aid  \\\n",
       "0            0                0        0              0     0     0         0   \n",
       "1            0                0        0              0     0     0         1   \n",
       "2            0                0        0              0     0     0         0   \n",
       "3            0                1        0              0     0     0         1   \n",
       "4            0                0        0              0     0     0         0   \n",
       "\n",
       "  other_infrastructure other_weather refugees related request  \\\n",
       "0                    0             0        0       1       0   \n",
       "1                    0             0        0       1       0   \n",
       "2                    0             0        0       1       0   \n",
       "3                    0             0        0       1       1   \n",
       "4                    0             0        0       1       0   \n",
       "\n",
       "  search_and_rescue security shelter shops storm tools transport water  \\\n",
       "0                 0        0       0     0     0     0         0     0   \n",
       "1                 0        0       0     0     1     0         0     0   \n",
       "2                 0        0       0     0     0     0         0     0   \n",
       "3                 0        0       0     0     0     0         0     0   \n",
       "4                 0        0       0     0     0     0         0     0   \n",
       "\n",
       "  weather_related  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary cleaning\n",
    "df.iloc[:,4:] = df.iloc[:,4:].astype('int64')\n",
    "df.drop('related', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenize(sentence):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    return [lemmatizer.lemmatize(word).lower().strip() for word in nltk.word_tokenize(sentence)\\\n",
    "            if not word in stop_words] \n",
    "\n",
    "message = df.message.apply(lambda x: \" \".join(tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather update cold front cuba could pas haiti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message[:10][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes are very imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26216, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>buildings</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>clothing</th>\n",
       "      <th>cold</th>\n",
       "      <th>death</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>electricity</th>\n",
       "      <th>fire</th>\n",
       "      <th>floods</th>\n",
       "      <th>food</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>infrastructure_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>military</th>\n",
       "      <th>missing_people</th>\n",
       "      <th>money</th>\n",
       "      <th>offer</th>\n",
       "      <th>other_aid</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>refugees</th>\n",
       "      <th>request</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>shelter</th>\n",
       "      <th>shops</th>\n",
       "      <th>storm</th>\n",
       "      <th>tools</th>\n",
       "      <th>transport</th>\n",
       "      <th>water</th>\n",
       "      <th>weather_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25907</td>\n",
       "      <td>15356</td>\n",
       "      <td>24883</td>\n",
       "      <td>26216.0</td>\n",
       "      <td>25811</td>\n",
       "      <td>25686</td>\n",
       "      <td>25022</td>\n",
       "      <td>21141</td>\n",
       "      <td>23761</td>\n",
       "      <td>25684</td>\n",
       "      <td>25934</td>\n",
       "      <td>24061</td>\n",
       "      <td>23293</td>\n",
       "      <td>25933</td>\n",
       "      <td>24511</td>\n",
       "      <td>24132</td>\n",
       "      <td>24903</td>\n",
       "      <td>25356</td>\n",
       "      <td>25918</td>\n",
       "      <td>25612</td>\n",
       "      <td>26098</td>\n",
       "      <td>22770</td>\n",
       "      <td>25065</td>\n",
       "      <td>24840</td>\n",
       "      <td>25341</td>\n",
       "      <td>21742</td>\n",
       "      <td>25492</td>\n",
       "      <td>25745</td>\n",
       "      <td>23902</td>\n",
       "      <td>26096</td>\n",
       "      <td>23773</td>\n",
       "      <td>26057</td>\n",
       "      <td>25015</td>\n",
       "      <td>24544</td>\n",
       "      <td>18919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309</td>\n",
       "      <td>10860</td>\n",
       "      <td>1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405</td>\n",
       "      <td>530</td>\n",
       "      <td>1194</td>\n",
       "      <td>5075</td>\n",
       "      <td>2455</td>\n",
       "      <td>532</td>\n",
       "      <td>282</td>\n",
       "      <td>2155</td>\n",
       "      <td>2923</td>\n",
       "      <td>283</td>\n",
       "      <td>1705</td>\n",
       "      <td>2084</td>\n",
       "      <td>1313</td>\n",
       "      <td>860</td>\n",
       "      <td>298</td>\n",
       "      <td>604</td>\n",
       "      <td>118</td>\n",
       "      <td>3446</td>\n",
       "      <td>1151</td>\n",
       "      <td>1376</td>\n",
       "      <td>875</td>\n",
       "      <td>4474</td>\n",
       "      <td>724</td>\n",
       "      <td>471</td>\n",
       "      <td>2314</td>\n",
       "      <td>120</td>\n",
       "      <td>2443</td>\n",
       "      <td>159</td>\n",
       "      <td>1201</td>\n",
       "      <td>1672</td>\n",
       "      <td>7297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aid_centers  aid_related  buildings  child_alone  clothing   cold  death  \\\n",
       "0        25907        15356      24883      26216.0     25811  25686  25022   \n",
       "1          309        10860       1333          NaN       405    530   1194   \n",
       "\n",
       "   direct_report  earthquake  electricity   fire  floods   food  hospitals  \\\n",
       "0          21141       23761        25684  25934   24061  23293      25933   \n",
       "1           5075        2455          532    282    2155   2923        283   \n",
       "\n",
       "   infrastructure_related  medical_help  medical_products  military  \\\n",
       "0                   24511         24132             24903     25356   \n",
       "1                    1705          2084              1313       860   \n",
       "\n",
       "   missing_people  money  offer  other_aid  other_infrastructure  \\\n",
       "0           25918  25612  26098      22770                 25065   \n",
       "1             298    604    118       3446                  1151   \n",
       "\n",
       "   other_weather  refugees  request  search_and_rescue  security  shelter  \\\n",
       "0          24840     25341    21742              25492     25745    23902   \n",
       "1           1376       875     4474                724       471     2314   \n",
       "\n",
       "   shops  storm  tools  transport  water  weather_related  \n",
       "0  26096  23773  26057      25015  24544            18919  \n",
       "1    120   2443    159       1201   1672             7297  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.iloc[:,4:].apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            0\n",
       "message                       0\n",
       "original                  16046\n",
       "genre                         0\n",
       "aid_centers                   0\n",
       "aid_related                   0\n",
       "buildings                     0\n",
       "child_alone                   0\n",
       "clothing                      0\n",
       "cold                          0\n",
       "death                         0\n",
       "direct_report                 0\n",
       "earthquake                    0\n",
       "electricity                   0\n",
       "fire                          0\n",
       "floods                        0\n",
       "food                          0\n",
       "hospitals                     0\n",
       "infrastructure_related        0\n",
       "medical_help                  0\n",
       "medical_products              0\n",
       "military                      0\n",
       "missing_people                0\n",
       "money                         0\n",
       "offer                         0\n",
       "other_aid                     0\n",
       "other_infrastructure          0\n",
       "other_weather                 0\n",
       "refugees                      0\n",
       "request                       0\n",
       "search_and_rescue             0\n",
       "security                      0\n",
       "shelter                       0\n",
       "shops                         0\n",
       "storm                         0\n",
       "tools                         0\n",
       "transport                     0\n",
       "water                         0\n",
       "weather_related               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_text(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        return pd.Series(X).apply(lambda x: \" \".join(self.tokenize(x))).values\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        #stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "        return [lemmatizer.lemmatize(word).lower().strip() for word in nltk.word_tokenize(sentence)\\\n",
    "            if not word in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weather update cold front cuba could pas haiti', 'is hurricane',\n",
       "       'looking someone name',\n",
       "       'un report leogane destroyed only hospital st croix functioning needs supply desperately',\n",
       "       'say west side haiti rest country today tonight'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = Clean_text()\n",
    "clean_text.transform(df.message)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline = Pipeline([\n",
    "    ('clean_text', Clean_text()),\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('abclf', MultiOutputClassifier(AdaBoostClassifier()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('clean_text', Clean_text()), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=...or=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.message.copy()\n",
    "y = df.iloc[:,4:].values \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32) # training\n",
    "X_train.shape, y_train.shape\n",
    "ml_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because classes are imbalanced, accuracy is not a good metric for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23313    Forty-eight teams of medical and paramedical s...\n",
       "7683                   NOTES: this message is not complete\n",
       "23462    All graphs provide evidence that, considering ...\n",
       "22334    construction material: bricks - 500 t; cement ...\n",
       "6522     I can arrange shelter shelter, i live out in t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X_train, y_train, pipeline):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_indices, test_indices in kf.split(X_train, y_train):\n",
    "   \n",
    "        X_train_kf = X_train.iloc[train_indices]\n",
    "        y_train_kf = y_train[train_indices]\n",
    "    \n",
    "        X_test_kf = X_train.iloc[test_indices]\n",
    "        y_test_kf = y_train[test_indices]\n",
    "  \n",
    "        pipeline.fit(X_train_kf, y_train_kf)\n",
    "    \n",
    "                \n",
    "        labels.extend(y_test_kf)\n",
    "        preds.extend(pipeline.predict(X_test_kf))\n",
    "\n",
    "    return pd.DataFrame(labels), pd.DataFrame(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'would like to make food/ serve food'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-52008a6ec0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'M8[ns]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'would like to make food/ serve food'"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "labels_df, preds_df = cross_val(X_res, y_res, ml_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.38331031369442364,\n",
       " 'accuracy_score': 0.9518296504182446,\n",
       " 'precision_score': 0.5477374941118585,\n",
       " 'recall_score': 0.30803428577420877}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scoring(scoring_metric, labels_df, preds_df):\n",
    "    return np.mean(np.array([scoring_metric(labels_df.iloc[:,column], preds_df.iloc[:,column]) \n",
    "            for column in range(0,len(df.iloc[:,4:].columns))]))\n",
    "\n",
    "list_scores = [f1_score, accuracy_score, precision_score, recall_score]\n",
    "{str(metric.__name__):scoring(metric, labels_df, preds_df) for metric in list_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.369716694196754,\n",
       " 'accuracy_score': 0.9509698158439578,\n",
       " 'precision_score': 0.5393897866050884,\n",
       " 'recall_score': 0.29508791625414005}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = ml_pipeline.predict(X_test)\n",
    "\n",
    "labels_df = pd.DataFrame(y_test)\n",
    "preds_df = pd.DataFrame(y_preds)\n",
    "list_scores = [f1_score, accuracy_score, precision_score, recall_score]\n",
    "{str(metric.__name__):scoring(metric, labels_df, preds_df) for metric in list_scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('clean_text', Clean_text()),\n",
       "  ('vectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('abclf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "              n_jobs=None))],\n",
       " 'clean_text': Clean_text(),\n",
       " 'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'abclf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "            n_jobs=None),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 1.0,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'abclf__estimator__algorithm': 'SAMME.R',\n",
       " 'abclf__estimator__base_estimator': None,\n",
       " 'abclf__estimator__learning_rate': 1.0,\n",
       " 'abclf__estimator__n_estimators': 50,\n",
       " 'abclf__estimator__random_state': None,\n",
       " 'abclf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'abclf__n_jobs': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunom/.miniconda2/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] abclf__estimator__learning_rate=0.1, abclf__estimator__n_estimators=100 \n",
      "[CV]  abclf__estimator__learning_rate=0.1, abclf__estimator__n_estimators=100, score=0.43827778572450293, total= 1.8min\n",
      "[CV] abclf__estimator__learning_rate=0.1, abclf__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  abclf__estimator__learning_rate=0.1, abclf__estimator__n_estimators=100, score=0.4380543633762518, total= 1.9min\n",
      "[CV] abclf__estimator__learning_rate=0.2, abclf__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  6.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  abclf__estimator__learning_rate=0.2, abclf__estimator__n_estimators=100, score=0.4357030467744243, total= 1.8min\n",
      "[CV] abclf__estimator__learning_rate=0.2, abclf__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  8.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  abclf__estimator__learning_rate=0.2, abclf__estimator__n_estimators=100, score=0.43598912887998853, total= 1.8min\n",
      "[CV] abclf__estimator__learning_rate=0.2, abclf__estimator__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.1min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "#parameters = {\n",
    "#    'vectorizer__max_df': [1.0,2.0],\n",
    "#    'vectorizer__max_features': [None,5,10],\n",
    "#    'vectorizer__ngram_range': [(1, 1),(1,2)],\n",
    "#    'tfidf__norm': ['l2','l1'],\n",
    "#    'tfidf__smooth_idf': [True, False],\n",
    "#    'tfidf__sublinear_tf': [True, False],\n",
    "#    'abclf__estimator__learning_rate': [0.1,0.2,0.5],\n",
    "#    'abclf__estimator__n_estimators': [50,100]\n",
    "#}\n",
    "\n",
    "parameters = {\n",
    "    #'vectorizer__min_df': [1],\n",
    "    #'vectorizer__max_df': [1.0],\n",
    "    #'vectorizer__max_features': [None],\n",
    "    #'vectorizer__ngram_range': [(1,1)],\n",
    "    #'tfidf__norm': ['l2'],\n",
    "    #'tfidf__smooth_idf': [True],\n",
    "    #'tfidf__sublinear_tf': [False],\n",
    "    'abclf__estimator__learning_rate': [0.1,0.2],\n",
    "    'abclf__estimator__n_estimators': [100]\n",
    "}\n",
    "\n",
    "\n",
    "grid_clf = GridSearchCV(ml_pipeline, param_grid=parameters, verbose=10)\n",
    "\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_best = grid_clf.best_estimator_\n",
    "print(ml_pipeline_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model. Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.37569392022806103,\n",
       " 'accuracy_score': 0.9520213577421816,\n",
       " 'precision_score': 0.5482943938694825,\n",
       " 'recall_score': 0.3020466938134612}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_pipeline_best = ml_pipeline\n",
    "ml_pipeline_best.fit(X_train, y_train)\n",
    "y_preds = ml_pipeline_best.predict(X_test)\n",
    "\n",
    "labels_df = pd.DataFrame(y_test)\n",
    "preds_df = pd.DataFrame(y_preds)\n",
    "list_scores = [f1_score, accuracy_score, precision_score, recall_score]\n",
    "{str(metric.__name__):scoring(metric, labels_df, preds_df) for metric in list_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5244.0</td>\n",
       "      <td>3637</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>4802</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>4745</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>4728</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5239</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5244.0</td>\n",
       "      <td>5041</td>\n",
       "      <td>5244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1       2       3       4       5       6     7       8       9   \\\n",
       "0  5244.0  3637  5244.0  5244.0  5244.0  5244.0  5244.0  4802  5244.0  5244.0   \n",
       "1     NaN  1607     NaN     NaN     NaN     NaN     NaN   442     NaN     NaN   \n",
       "\n",
       "       10      11    12      13      14      15      16      17      18  \\\n",
       "0  5244.0  5244.0  4745  5244.0  5244.0  5244.0  5244.0  5244.0  5244.0   \n",
       "1     NaN     NaN   499     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       19      20      21      22      23      24    25      26      27    28  \\\n",
       "0  5244.0  5244.0  5244.0  5244.0  5244.0  5244.0  4728  5244.0  5244.0  5239   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN   516     NaN     NaN     5   \n",
       "\n",
       "       29      30      31      32    33      34  \n",
       "0  5244.0  5244.0  5244.0  5244.0  5041  5244.0  \n",
       "1     NaN     NaN     NaN     NaN   203     NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_text_with_stemmer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        return pd.Series(X).apply(lambda x: \" \".join(self.tokenize(x))).values\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "        return [stemmer.stem(lemmatizer.lemmatize(word).lower().strip()) for word in nltk.word_tokenize(sentence)\\\n",
    "            if not word in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_improved = Pipeline([\n",
    "    ('clean_text', Clean_text_with_stemmer()),\n",
    "    ('vectorizer', CountVectorizer(min_df= 5,stop_words=\"english\", ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer(sublinear_tf=True, norm='l2')),\n",
    "    ('mbclf', MultiOutputClassifier(LogisticRegression(random_state=34)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='ml_pipeline_improved.pkl'\n",
    "pickle.dump(ml_pipeline_improved, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "(20972, 36)\n",
      "(20972, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunom/.miniconda2/lib/python3.6/site-packages/sklearn/naive_bayes.py:465: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9837909992372235,\n",
       " 0.7688787185354691,\n",
       " 0.918001525553013,\n",
       " 1.0,\n",
       " 0.9780701754385965,\n",
       " 0.9702517162471396,\n",
       " 0.9328756674294432,\n",
       " 0.814836003051106,\n",
       " 0.933257055682685,\n",
       " 0.9687261632341724,\n",
       " 0.9858886346300534,\n",
       " 0.8983600305110603,\n",
       " 0.9109458428680397,\n",
       " 0.9832189168573608,\n",
       " 0.8785278413424866,\n",
       " 0.881578947368421,\n",
       " 0.9248665141113653,\n",
       " 0.9473684210526315,\n",
       " 0.9837909992372235,\n",
       " 0.9586193745232647,\n",
       " 0.9921815408085431,\n",
       " 0.8236079328756675,\n",
       " 0.914187643020595,\n",
       " 0.90255530129672,\n",
       " 0.9363081617086194,\n",
       " 0.7858504958047292,\n",
       " 0.8773836765827613,\n",
       " 0.9496567505720824,\n",
       " 0.9725400457665904,\n",
       " 0.906559877955759,\n",
       " 0.9937070938215103,\n",
       " 0.8991228070175439,\n",
       " 0.9916094584286804,\n",
       " 0.9204805491990846,\n",
       " 0.9294431731502669]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test y straight from df\n",
    "vectorizer = TfidfVectorizer(min_df= 5, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "vector_features = vectorizer.fit_transform(df_clean_message).toarray()\n",
    "final_features = pd.DataFrame(vectorizer.fit_transform(df_clean_message).toarray(), \n",
    "                              columns=vectorizer.get_feature_names())\n",
    "\n",
    "X = vector_features\n",
    "y2 = MultiLabelBinarizer().fit_transform(labels)\n",
    "y = df.iloc[:,4:].values\n",
    "print(type(y2[0][0]))\n",
    "print(type(y[0][0]))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "print(y_train.shape)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size=0.2, random_state=32)\n",
    "print(y_train2.shape)\n",
    "#clf = OneVsRestClassifier(SVC(probability=True))\n",
    "#clf = OneVsRestClassifier(BernoulliNB())\n",
    "clf = MultiOutputClassifier(BernoulliNB())\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "[accuracy_score(y_test[:,column],predictions[:,column]) for column in range(len(df.iloc[:,4:].columns)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1 - use Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aai</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abad</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abba</th>\n",
       "      <th>abdomin</th>\n",
       "      <th>abdomin pain</th>\n",
       "      <th>abduct</th>\n",
       "      <th>...</th>\n",
       "      <th>zhejiang provinc</th>\n",
       "      <th>zhouqu</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zinc roof</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoonot</th>\n",
       "      <th>zoonot diseas</th>\n",
       "      <th>zy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  11553 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aai   ab  ababa  abad  abandon  abba  abdomin  abdomin pain  abduct  \\\n",
       "0  0.0  0.0  0.0    0.0   0.0      0.0   0.0      0.0           0.0     0.0   \n",
       "1  0.0  0.0  0.0    0.0   0.0      0.0   0.0      0.0           0.0     0.0   \n",
       "2  0.0  0.0  0.0    0.0   0.0      0.0   0.0      0.0           0.0     0.0   \n",
       "3  0.0  0.0  0.0    0.0   0.0      0.0   0.0      0.0           0.0     0.0   \n",
       "4  0.0  0.0  0.0    0.0   0.0      0.0   0.0      0.0           0.0     0.0   \n",
       "\n",
       "   ...  zhejiang provinc  zhouqu  zimbabw  zimbabwean  zinc  zinc roof  zone  \\\n",
       "0  ...               0.0     0.0      0.0         0.0   0.0        0.0   0.0   \n",
       "1  ...               0.0     0.0      0.0         0.0   0.0        0.0   0.0   \n",
       "2  ...               0.0     0.0      0.0         0.0   0.0        0.0   0.0   \n",
       "3  ...               0.0     0.0      0.0         0.0   0.0        0.0   0.0   \n",
       "4  ...               0.0     0.0      0.0         0.0   0.0        0.0   0.0   \n",
       "\n",
       "   zoonot  zoonot diseas   zy  \n",
       "0     0.0            0.0  0.0  \n",
       "1     0.0            0.0  0.0  \n",
       "2     0.0            0.0  0.0  \n",
       "3     0.0            0.0  0.0  \n",
       "4     0.0            0.0  0.0  \n",
       "\n",
       "[5 rows x 11553 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26216\n",
      "(26216, 35)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9855524734165511,\n",
       " 0.7610957004160888,\n",
       " 0.926490984743412,\n",
       " 0.9808136846971799,\n",
       " 0.9737632917244567,\n",
       " 0.9379334257975035,\n",
       " 0.8206195099398983,\n",
       " 0.9248728617660656,\n",
       " 0.9730698104484512,\n",
       " 0.9880952380952381,\n",
       " 0.9010633379565418,\n",
       " 0.9054553860379103,\n",
       " 0.9849745723532132,\n",
       " 0.8839574664817383,\n",
       " 0.8861534905224225,\n",
       " 0.9290337494220989,\n",
       " 0.9545769764216366,\n",
       " 0.9845122515025427,\n",
       " 0.9637078132223763,\n",
       " 0.9931807674526121,\n",
       " 0.8305594082293112,\n",
       " 0.9218677762367082,\n",
       " 0.9082293111419325,\n",
       " 0.9467175219602404,\n",
       " 0.7894128525196487,\n",
       " 0.8769070735090153,\n",
       " 0.9549237170596394,\n",
       " 0.973994452149792,\n",
       " 0.9060332871012483,\n",
       " 0.9937586685159501,\n",
       " 0.9008321775312067,\n",
       " 0.9921405455386038,\n",
       " 0.9239482200647249,\n",
       " 0.929842810910772,\n",
       " 0.8413083680073972]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 5, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "vector_features = vectorizer.fit_transform(df_clean_message).toarray()\n",
    "final_features = pd.DataFrame(vectorizer.fit_transform(df_clean_message).toarray(), \n",
    "                              columns=vectorizer.get_feature_names())\n",
    "\n",
    "display(final_features.head())\n",
    "\n",
    "X = vector_features\n",
    "y = MultiLabelBinarizer().fit_transform(labels)\n",
    "print(len(labels))\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=32)\n",
    "\n",
    "#clf = OneVsRestClassifier(SVC(probability=True))\n",
    "clf = OneVsRestClassifier(BernoulliNB())\n",
    "#clf = OneVsRestClassifier(AdaBoostClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "[accuracy_score(y_test[:,column],predictions[:,column]) for column in range(len(df.iloc[:,4:].columns)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: use CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aai</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abad</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abba</th>\n",
       "      <th>abdomin</th>\n",
       "      <th>abdomin pain</th>\n",
       "      <th>abduct</th>\n",
       "      <th>...</th>\n",
       "      <th>zhejiang provinc</th>\n",
       "      <th>zhouqu</th>\n",
       "      <th>zimbabw</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zinc roof</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoonot</th>\n",
       "      <th>zoonot diseas</th>\n",
       "      <th>zy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  11553 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aai  ab  ababa  abad  abandon  abba  abdomin  abdomin pain  abduct  \\\n",
       "0   0    0   0      0     0        0     0        0             0       0   \n",
       "1   0    0   0      0     0        0     0        0             0       0   \n",
       "2   0    0   0      0     0        0     0        0             0       0   \n",
       "3   0    0   0      0     0        0     0        0             0       0   \n",
       "4   0    0   0      0     0        0     0        0             0       0   \n",
       "\n",
       "   ...  zhejiang provinc  zhouqu  zimbabw  zimbabwean  zinc  zinc roof  zone  \\\n",
       "0  ...                 0       0        0           0     0          0     0   \n",
       "1  ...                 0       0        0           0     0          0     0   \n",
       "2  ...                 0       0        0           0     0          0     0   \n",
       "3  ...                 0       0        0           0     0          0     0   \n",
       "4  ...                 0       0        0           0     0          0     0   \n",
       "\n",
       "   zoonot  zoonot diseas  zy  \n",
       "0       0              0   0  \n",
       "1       0              0   0  \n",
       "2       0              0   0  \n",
       "3       0              0   0  \n",
       "4       0              0   0  \n",
       "\n",
       "[5 rows x 11553 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26216\n",
      "(26216, 35)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9855524734165511,\n",
       " 0.7610957004160888,\n",
       " 0.926490984743412,\n",
       " 0.9808136846971799,\n",
       " 0.9737632917244567,\n",
       " 0.9379334257975035,\n",
       " 0.8206195099398983,\n",
       " 0.9248728617660656,\n",
       " 0.9730698104484512,\n",
       " 0.9880952380952381,\n",
       " 0.9010633379565418,\n",
       " 0.9054553860379103,\n",
       " 0.9849745723532132,\n",
       " 0.8839574664817383,\n",
       " 0.8861534905224225,\n",
       " 0.9290337494220989,\n",
       " 0.9545769764216366,\n",
       " 0.9845122515025427,\n",
       " 0.9637078132223763,\n",
       " 0.9931807674526121,\n",
       " 0.8305594082293112,\n",
       " 0.9218677762367082,\n",
       " 0.9082293111419325,\n",
       " 0.9467175219602404,\n",
       " 0.7894128525196487,\n",
       " 0.8769070735090153,\n",
       " 0.9549237170596394,\n",
       " 0.973994452149792,\n",
       " 0.9060332871012483,\n",
       " 0.9937586685159501,\n",
       " 0.9008321775312067,\n",
       " 0.9921405455386038,\n",
       " 0.9239482200647249,\n",
       " 0.929842810910772,\n",
       " 0.8413083680073972]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df= 5,stop_words=\"english\", ngram_range=(1, 2))\n",
    "vector_features = vectorizer.fit_transform(df_clean_message).toarray()\n",
    "final_features = pd.DataFrame(vectorizer.fit_transform(df_clean_message).toarray(), \n",
    "                              columns=vectorizer.get_feature_names())\n",
    "\n",
    "display(final_features.head())\n",
    "\n",
    "X = vector_features\n",
    "y = MultiLabelBinarizer().fit_transform(labels)\n",
    "print(len(labels))\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=32)\n",
    "\n",
    "#clf = OneVsRestClassifier(SVC(probability=True))\n",
    "clf = OneVsRestClassifier(BernoulliNB())\n",
    "#clf = OneVsRestClassifier(AdaBoostClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "[accuracy_score(y_test[:,column],predictions[:,column]) for column in range(len(df.iloc[:,4:].columns)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26216, 35)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = vector_features\n",
    "y2 = df.iloc[:,4:-1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.33, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2311604253351826"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(sentence):\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    return [word for word in nltk.word_tokenize(sentence)\\\n",
    "            if not word in stop_words] \n",
    "df_clean_message = df.message.apply(lambda x: \" \".join(tokenize(x)))\n",
    "#vectorizer = TfidfVectorizer(min_df= 5, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "vectorizer = TfidfVectorizer(min_df= 5, stop_words=\"english\", sublinear_tf=True, norm='l1', ngram_range=(1, 2))\n",
    "vector_features = vectorizer.fit_transform(df_clean_message).toarray()\n",
    "X = vector_features\n",
    "y = MultiLabelBinarizer().fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=32)\n",
    "\n",
    "#clf = OneVsRestClassifier(SVC(probability=True))\n",
    "#clf = OneVsRestClassifier(BernoulliNB())\n",
    "clf = OneVsRestClassifier(AdaBoostClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_trainb\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aid_centers                 309\n",
       "aid_related               10860\n",
       "buildings                  1333\n",
       "child_alone                   0\n",
       "clothing                    405\n",
       "cold                        530\n",
       "death                      1194\n",
       "direct_report              5075\n",
       "earthquake                 2455\n",
       "electricity                 532\n",
       "fire                        282\n",
       "floods                     2155\n",
       "food                       2923\n",
       "hospitals                   283\n",
       "infrastructure_related     1705\n",
       "medical_help               2084\n",
       "medical_products           1313\n",
       "military                    860\n",
       "missing_people              298\n",
       "money                       604\n",
       "offer                       118\n",
       "other_aid                  3446\n",
       "other_infrastructure       1151\n",
       "other_weather              1376\n",
       "refugees                    875\n",
       "related                   20282\n",
       "request                    4474\n",
       "search_and_rescue           724\n",
       "security                    471\n",
       "shelter                    2314\n",
       "shops                       120\n",
       "storm                      2443\n",
       "tools                       159\n",
       "transport                  1201\n",
       "water                      1672\n",
       "weather_related            7297\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,4:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[:,4:].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878640776699029"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[:,0], predictions[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8652, 35)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9878640776699029,\n",
       " 0.7457235321312992,\n",
       " 0.9617429496070273,\n",
       " 0.9872861766065649,\n",
       " 0.9843966712898752,\n",
       " 0.9664817383263985,\n",
       " 0.8542533518261674,\n",
       " 0.9696024040684235,\n",
       " 0.9812760055478502,\n",
       " 0.9912159038372631,\n",
       " 0.9555016181229773,\n",
       " 0.9435968562182154,\n",
       " 0.9864771151178918,\n",
       " 0.9294960702727693,\n",
       " 0.9254507628294036,\n",
       " 0.9526121128062875,\n",
       " 0.9699491447064262,\n",
       " 0.9878640776699029,\n",
       " 0.9769995376791494,\n",
       " 0.9939898289412853,\n",
       " 0.8646555709662506,\n",
       " 0.9520342117429496,\n",
       " 0.9484512251502543,\n",
       " 0.9684466019417476,\n",
       " 0.7786638927415627,\n",
       " 0.8919325011558021,\n",
       " 0.970873786407767,\n",
       " 0.9823162274618585,\n",
       " 0.9469486823855756,\n",
       " 0.994914470642626,\n",
       " 0.9348127600554785,\n",
       " 0.9942209893666204,\n",
       " 0.9586222838650024,\n",
       " 0.9605871474803513,\n",
       " 0.8788719371243643]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[:,4:].columns)\n",
    "[accuracy_score(y_test[:,column],predictions[:,column]) for column in range(len(df.iloc[:,4:].columns)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-8b5b9c2f7b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# overall.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mShape\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mproblems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \"\"\"\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'multioutput'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             raise ValueError(\"Multioutput target data is not supported with \"\n\u001b[0m\u001b[1;32m    408\u001b[0m                              \"label binarization\")\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) \n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (17564, 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-2b3eb7dc5e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m                         dtype=None)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda2/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (17564, 37)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf = BernoulliNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautiful solution\n",
    "#df.message.apply(lambda x: \" \".join([stemmer.stem(word) for word in re.sub(\"[^a-zA-Z]\", \" \", x).split() \n",
    "#          if word not in stop_words]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Select_features(BaseEstimator, TransformerMixin):\n",
    "    # constructor\n",
    "    def __init__(self, column_names):\n",
    "        self.column_names = column_names \n",
    "    \n",
    "    # pipeline must implement fit\n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "    \n",
    "    # return new dataframe with selected features\n",
    "    def transform(self, X=None, y=None):\n",
    "        return X[self.column_names] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vector_features\n",
    "#y = MultiLabelBinarizer().fit_transform(labels)\n",
    "#print(len(labels))\n",
    "#print(y.shape)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=32)\n",
    "#\n",
    "#vectorizer = CountVectorizer(min_df= 5,stop_words=\"english\", ngram_range=(1, 2))\n",
    "#clf = OneVsRestClassifier(BernoulliNB())\n",
    "#message_pipeline = Pipeline([('vectorizer', vectorizer),('clf',clf)])\n",
    "#message_pipeline.fit()\n",
    "#message_pipeline.named_steps['vectorizer'].transform([df_clean_message])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
